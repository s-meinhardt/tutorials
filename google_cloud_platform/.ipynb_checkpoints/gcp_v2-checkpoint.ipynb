{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MNIST Classifier locally and on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this tutorial is to set up a Python package for a MNIST Classifier which can be trained and deployed on Google Cloud Platform (GCP). We close this tutorial by requesting predictions from our model on GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Writing a Python package containing our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating some Python variables. By convention, we will use small names for Python variables and capital names for Shell variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "package_path = 'trainer'\n",
    "module_name = 'task'\n",
    "model_dir = 'mnist_classifier'\n",
    "\n",
    "init_path = os.path.join(package_path,'__init__.py')  # path to the __init__.py file \n",
    "module_path = os.path.join(package_path, module_name+'.py') # path to the task.py file\n",
    "model_path = os.path.join(package_path, 'model.py') # path to the model.py file\n",
    "\n",
    "os.environ['PACKAGE_PATH'] = package_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Python package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python package is a directory which contains a typically empty `__init__.py` file along with some Python files containing the actual code. First, we create a directory which will become our Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(package_path):\n",
    "    print('The path already exists. Shall we delete its content? (y/n)')\n",
    "    answer = input()\n",
    "    if answer == 'y' or answer == 'Y':\n",
    "        shutil.rmtree(package_path, ignore_errors = True)\n",
    "        os.mkdir(package_path)\n",
    "else:        \n",
    "    os.mkdir(package_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an empty `__init__.py` file inside our package directory to define this directory as a python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $init_path   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `model.py` file containing the model code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $model_path   \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def input_fn(features, labels, shuffle = True, epochs = None, batch_size = 128):\n",
    "    \n",
    "    if labels is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "        \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs).shuffle(10000).cache()\n",
    "    dataset = dataset.batch(batch_size).repeat(epochs).prefetch(AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def build_model(input_shape, learning_rate):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape(target_shape=[28,28,1], input_shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr = learning_rate)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `task.py` file containing the code to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $module_path   \n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "# Importing job specific libraries\n",
    "from .model import input_fn, build_model\n",
    "   \n",
    "\n",
    "\n",
    "def get_arguments():\n",
    "    \n",
    "    parser = ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--gpu_memory_growth',\n",
    "                        type = bool, \n",
    "                        default = False,\n",
    "                        help = 'whether or not gpu memory growth will be enabled')\n",
    "    \n",
    "    parser.add_argument( '--shuffle',\n",
    "                        type = bool, \n",
    "                        default = True,\n",
    "                        help = 'whether to shuffle the input samples or not')\n",
    "    \n",
    "    parser.add_argument('--epochs',\n",
    "                        type = int,\n",
    "                        default = 100,\n",
    "                        help = 'the maximal number of epochs during training')\n",
    "    \n",
    "    parser.add_argument('--batch_size',\n",
    "                        type = int,\n",
    "                        default = 128,\n",
    "                        help = 'how many samples should be processed during one training step')\n",
    "    \n",
    "    parser.add_argument('--learning_rate',\n",
    "                        type = float, \n",
    "                        default = 0.01,\n",
    "                        help = 'the initial learning rate of the Adam optimizer')\n",
    "    \n",
    "    parser.add_argument('--model_dir',\n",
    "                        type = str, \n",
    "                        required = True,\n",
    "                        help = 'where to store checkpoints and the exported model')\n",
    "    \n",
    "    parser.add_argument('--log_dir',\n",
    "                        type = str,\n",
    "                        required = True,\n",
    "                        help = 'where to store logs needed for Tensorboard')\n",
    "    \n",
    "    parser.add_argument('--ckpt_dir',\n",
    "                        type = str,\n",
    "                        required = True,\n",
    "                        help = 'where to store the checkpoints')\n",
    "\n",
    "    parser.add_argument('--verbose',\n",
    "                        type = int,\n",
    "                        default = 0,\n",
    "                        choices = [0,1,2])    \n",
    "    \n",
    "    parser.add_argument('--verbosity',\n",
    "                        type = str,\n",
    "                        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "                        default='INFO')\n",
    "    \n",
    "    args, _ =  parser.parse_known_args()\n",
    "    return args\n",
    "    \n",
    "    \n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    \n",
    "    # Enable memory growth if training is done locally\n",
    "    if args.gpu_memory_growth:\n",
    "        for gpu in tf.config.list_physical_devices('GPU'):\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "    \n",
    "    # Loading the data\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # Create a training dataset\n",
    "    train_dataset = input_fn(x_train, y_train, shuffle=args.shuffle, epochs=args.epochs, batch_size=args.batch_size)\n",
    "    \n",
    "    # Create a validation dataset\n",
    "    valid_dataset = input_fn(x_test, y_test, shuffle=False, epochs=1, batch_size=args.batch_size)\n",
    "    \n",
    "    \n",
    "    # Building the model\n",
    "    input_shape = x_train[0].shape\n",
    "    model = build_model(input_shape = input_shape, learning_rate = args.learning_rate)\n",
    "    \n",
    "    \n",
    "    # Defining some callbacks\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor = 0.1, \n",
    "                                                     monitor = 'val_accuracy', \n",
    "                                                     patience = 10)\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir = args.log_dir)\n",
    "    checkpoints = tf.keras.callbacks.ModelCheckpoint(filepath = args.ckpt_dir, \n",
    "                                                     monitor = 'val_accuracy', \n",
    "                                                     save_best_only = True,\n",
    "                                                     save_weights_only = True)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
    "                                                      min_delta = 0.005,\n",
    "                                                      patience = 20)\n",
    "    callbacks = [reduce_lr, tensorboard, checkpoints, early_stopping]\n",
    "    \n",
    "    \n",
    "    # Starting the training\n",
    "    steps_per_epoch = x_train.shape[0] // args.batch_size\n",
    "    history = model.fit(train_dataset, \n",
    "                        validation_data = valid_dataset, \n",
    "                        steps_per_epoch = steps_per_epoch,\n",
    "                        epochs = args.epochs,\n",
    "                        callbacks = callbacks,\n",
    "                        verbose = args.verbose)\n",
    "    \n",
    "    # Loading the best weights into the model\n",
    "    model.load_weights(args.ckpt_dir)\n",
    "    \n",
    "    # Saving the model\n",
    "    model_path = os.path.join(args.model_dir)\n",
    "    model.save(model_path)\n",
    "    print(f'Keras model exported to {args.model_dir}')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    args = get_arguments()\n",
    "    tf.compat.v1.logging.set_verbosity(args.verbosity)\n",
    "    train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train our model with the `gcloud` command line tool, we need to export some variables to make them available in the Shell. We will use small variable names for Python variables and capital names for Shell variabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MODULE_NAME'] = package_path + '.' + module_name  # Note the 'dot' between the package name and the module name\n",
    "os.environ['MODEL_DIR'] = model_dir\n",
    "os.environ['LOG_DIR'] = 'logs'\n",
    "os.environ['CKPT_DIR'] = 'checkpoints/weights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start to train our model locally using the command line tool `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "gcloud ai-platform local train \\\n",
    "    --package-path $PACKAGE_PATH \\\n",
    "    --module-name $MODULE_NAME \\\n",
    "    --job-dir $MODEL_DIR \\\n",
    "    -- \\\n",
    "    --gpu_memory_growth True \\\n",
    "    --model_dir $MODEL_DIR \\\n",
    "    --log_dir $MODEL_DIR/$LOG_DIR \\\n",
    "    --ckpt_dir $MODEL_DIR/$CKPT_DIR \\\n",
    "    --verbose 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we define some variables and export them. The `GOOGLE_APPLICATION_CREDENTIALS` are used by the Google API's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some project variables\n",
    "credentials = # path to your service account key file\n",
    "project_id = # your project id\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials\n",
    "os.environ['PROJECT_ID'] = project_id\n",
    "\n",
    "# Some bucket variables\n",
    "bucket_name = 'gs://' + project_id + '-2nd-bucket'\n",
    "region = 'europe-west2'\n",
    "os.environ['BUCKET_NAME'] = bucket_name\n",
    "os.environ['BUCKET_REGION'] = region\n",
    "\n",
    "# Some job variables\n",
    "os.environ['JOB_NAME'] = model_dir + '_v2'\n",
    "os.environ['JOB_REGION'] = region   # It is always good to choose the bucket region to avoid latency\n",
    "os.environ['JOB_DIR'] = bucket_name + '/' + model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not already done, we create a bucket on Google Cloud Storage to hold our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "\n",
    "#gsutil mb -l $BUCKET_REGION -p $PROJECT_ID $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to send a training job to GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "    --package-path $PACKAGE_PATH \\\n",
    "    --module-name $MODULE_NAME \\\n",
    "    --region $JOB_REGION \\\n",
    "    --python-version 3.7 \\\n",
    "    --runtime-version 2.1 \\\n",
    "    --job-dir $JOB_DIR \\\n",
    "    -- \\\n",
    "    --model_dir $JOB_DIR \\\n",
    "    --log_dir $JOB_DIR/$LOG_DIR \\\n",
    "    --ckpt_dir $JOB_DIR/$CKPT_DIR \\\n",
    "    --verbose 0\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Set up a model on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up some variables. As before, small names will be used for Python variables and capital names for Shell variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mnist_classifier'\n",
    "model_version = 'v0001'\n",
    "model_region = 'europe-west1'  # 'europe-west2' is not available\n",
    "\n",
    "os.environ['MODEL_NAME'] = model_name\n",
    "os.environ['VERSION'] = model_version\n",
    "os.environ['MODEL_REGION'] = model_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command creates a model. However, the actual creation happens in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud ai-platform models create $MODEL_NAME \\\n",
    "    --regions $MODEL_REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a model version. It is this step when the model is finally created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud ai-platform versions create $VERSION \\\n",
    "    --model $MODEL_NAME \\\n",
    "    --runtime-version 2.1 \\\n",
    "    --python-version 3.7 \\\n",
    "    --framework tensorflow \\\n",
    "    --origin $JOB_DIR\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Prediction with models on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with gcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll send a json file using the command line tool `gcloud`. Each row in the json file is one individual sample. The result will be shown on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('prediction_input.json', 'w') as json_file:\n",
    "    for image in x_test[:3]:\n",
    "        json.dump(image.tolist(), json_file)\n",
    "        json_file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "gcloud ai-platform predict \\\n",
    "    --model $MODEL_NAME \\\n",
    "    --version $VERSION \\\n",
    "    --json-instances prediction_input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with googleapiclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a prediction within Python we need a different strategy. This allows us to save the results in a variable which can be used elsewhere. **Note** that we need to export `GOOGLE_APPLICATION_CREDENTIALS` before we can send the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1').projects()\n",
    "\n",
    "name = f'projects/{project_id}/models/{model_name}/versions/{model_version}'\n",
    "body = {'signature': 'serving_default',  # This is optional\n",
    "        'instances': x_test[:3].tolist()}\n",
    "\n",
    "response = service.predict(name = name, body = body).execute()  # Don't forget to call  'execute()'\n",
    "\n",
    "if 'error' in response:\n",
    "    raise RuntimeError(response['error'])\n",
    "\n",
    "response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3_env",
   "language": "python",
   "name": "project3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
